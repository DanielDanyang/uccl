# TCPX Unpack æ¶æ„åˆ†æä¸æ›´æ–°è®¡åˆ’

## ğŸ“‹ **èƒŒæ™¯ä¸é—®é¢˜åˆ†æ**

æ ¹æ® GCP NCCL-TCPX å›¢é˜Ÿçš„å›å¤ï¼Œæˆ‘ä»¬ç°åœ¨æ˜ç¡®äº†é—®é¢˜çš„æ ¹æœ¬åŸå› ï¼š

### ğŸ” **æ ¸å¿ƒé—®é¢˜**
1. **TCPXæ’ä»¶ä¸ç›´æ¥å†™å…¥ç”¨æˆ·ç¼“å†²åŒº**ï¼š`tcpxIrecv_v5` ä¸ä¼šç›´æ¥å°†æ•°æ®å†™å…¥ `void *data` å­—æ®µ
2. **éœ€è¦NCCLçš„unpackå†…æ ¸**ï¼šæ•°æ®æ¥æ”¶åéœ€è¦NCCLçš„è®¾å¤‡ç«¯å†…æ ¸æ¥æ‰§è¡Œ"unpack"æ“ä½œ
3. **æˆ‘ä»¬çš„æµ‹è¯•ç¼ºå°‘unpackæ­¥éª¤**ï¼šç‹¬ç«‹æµ‹è¯•ä¸­æ²¡æœ‰è¿è¡ŒNCCLçš„è®¾å¤‡ç«¯å†…æ ¸ï¼Œå¯¼è‡´GPUç¼“å†²åŒºä¿æŒæœªä¿®æ”¹çŠ¶æ€

### ğŸ—ï¸ **TCPXæ¥æ”¶æ¶æ„**
```
ç½‘ç»œæ•°æ® â†’ NIC â†’ GPUå†…å­˜é¡µé¢ â†’ unpacké˜Ÿåˆ—å…ƒæ•°æ® â†’ NCCLè®¾å¤‡å†…æ ¸ â†’ ç”¨æˆ·ç¼“å†²åŒº
```

## ğŸ”¬ **æŠ€æœ¯æ¶æ„åˆ†æ**

### 1. **NCCL Unpack å†…æ ¸æœºåˆ¶**

æ ¹æ® NCCL æºç åˆ†æï¼š
- **ä½ç½®**: `src/device/network/unpack/`
- **è°ƒç”¨ç‚¹**: `src/device/prims_simple.h:242`
- **åŠŸèƒ½**: å°†åˆ†æ•£çš„æ•°æ®åŒ…ç¼“å†²åŒºåˆ—è¡¨å¤åˆ¶åˆ°è¿ç»­çš„ç”¨æˆ·æä¾›çš„å¼ é‡ç¼“å†²åŒº

### 2. **Linux Kernel DevMem-TCP API**

æ ¹æ® Linux å†…æ ¸æ–‡æ¡£ (https://docs.kernel.org/networking/devmem.html)ï¼š
- **æ ¸å¿ƒæœºåˆ¶**: é€šè¿‡ `recvmsg()` ç³»ç»Ÿè°ƒç”¨çš„ `cmsg` ä¼ é€’åˆ†æ•£åˆ—è¡¨
- **æ§åˆ¶æ¶ˆæ¯ç±»å‹**:
  - `SCM_DEVMEM_DMABUF`: æ•°æ®è½åœ¨dmabufä¸­
  - `SCM_DEVMEM_LINEAR`: æ•°æ®è½åœ¨çº¿æ€§ç¼“å†²åŒºä¸­
- **æ•°æ®ç»“æ„**: `struct dmabuf_cmsg` åŒ…å«åç§»é‡ã€å¤§å°å’Œä»¤ç‰Œä¿¡æ¯

### 3. **TCPXæ’ä»¶å®ç°ç»†èŠ‚**

åŸºäºæºç åˆ†æï¼š
- **GPUæ¥æ”¶è·¯å¾„**: `gpudirectTCPXRecv()` â†’ `process_recv_cmsg()` â†’ unpacké˜Ÿåˆ—
- **å…ƒæ•°æ®å¤„ç†**: å°† `scatter_list` å¤åˆ¶åˆ° `unpack_slot.mem`
- **è®¾å¤‡é˜Ÿåˆ—**: ä½¿ç”¨ `tcpxNetDeviceQueue` ç®¡ç†unpackä»»åŠ¡

## ğŸ¯ **è§£å†³æ–¹æ¡ˆè®¾è®¡**

### **æ–¹æ¡ˆA: å®ç°è‡ªå®šä¹‰Unpackå†…æ ¸ (æ¨è)**

#### ä¼˜åŠ¿
- å®Œå…¨æ§åˆ¶unpacké€»è¾‘
- å¯ä»¥ä¼˜åŒ–æ€§èƒ½
- ç‹¬ç«‹äºNCCLå†…éƒ¨å®ç°

#### å®ç°æ­¥éª¤
1. **è§£æunpackå…ƒæ•°æ®**ï¼šä»TCPXæ’ä»¶è·å–scatter_list
2. **ç¼–å†™CUDAå†…æ ¸**ï¼šå®ç°æ•°æ®ä»åˆ†æ•£é¡µé¢åˆ°è¿ç»­ç¼“å†²åŒºçš„å¤åˆ¶
3. **é›†æˆåˆ°æµ‹è¯•**ï¼šåœ¨æ¥æ”¶å®Œæˆåè°ƒç”¨è‡ªå®šä¹‰unpackå†…æ ¸

### **æ–¹æ¡ˆB: ä½¿ç”¨HOSTå†…å­˜æ¥æ”¶ (ä¸´æ—¶æ–¹æ¡ˆ)**

#### ä¼˜åŠ¿
- ç»•è¿‡GPU unpacké˜Ÿåˆ—é—®é¢˜
- å¿«é€ŸéªŒè¯TCPXä¼ è¾“åŠŸèƒ½
- å®ç°ç®€å•

#### é™åˆ¶
- æ— æ³•éªŒè¯çœŸæ­£çš„GPU Directä¼ è¾“
- æ€§èƒ½ä¸æ˜¯æœ€ä¼˜çš„

## ğŸ“ **è¯¦ç»†å®ç°è®¡åˆ’**

### **é˜¶æ®µ1: ç†è§£å’ŒéªŒè¯ (1-2å¤©)**

#### 1.1 æ·±å…¥åˆ†æNCCL unpackæœºåˆ¶
- [ ] è·å–NCCL unpackå†…æ ¸æºç 
- [ ] åˆ†æ `loadMeta` æ•°æ®ç»“æ„
- [ ] ç†è§£scatter-gatheråˆ°è¿ç»­å†…å­˜çš„æ˜ å°„

#### 1.2 åˆ†æTCPX unpacké˜Ÿåˆ—
- [ ] ç ”ç©¶ `unpack_slot` æ•°æ®ç»“æ„
- [ ] ç†è§£ `scatter_list` æ ¼å¼
- [ ] åˆ†æå…ƒæ•°æ®ä¼ é€’æœºåˆ¶

#### 1.3 éªŒè¯DevMem-TCP API
- [ ] ç ”ç©¶Linuxå†…æ ¸devmem-tcpæ–‡æ¡£
- [ ] åˆ†æ `struct dmabuf_cmsg` ç»“æ„
- [ ] ç†è§£ä»¤ç‰Œç®¡ç†æœºåˆ¶

### **é˜¶æ®µ2: è®¾è®¡è‡ªå®šä¹‰Unpackå†…æ ¸ (2-3å¤©)**

#### 2.1 æ•°æ®ç»“æ„è®¾è®¡
```c
// è‡ªå®šä¹‰unpackå…ƒæ•°æ®ç»“æ„
struct CustomUnpackMeta {
    uint32_t src_offset;    // æºé¡µé¢åç§»
    uint32_t dst_offset;    // ç›®æ ‡ç¼“å†²åŒºåç§»  
    uint32_t length;        // æ•°æ®é•¿åº¦
    void* src_page_ptr;     // æºé¡µé¢æŒ‡é’ˆ
};

// Unpackä»»åŠ¡æè¿°ç¬¦
struct UnpackTask {
    void* dst_buffer;              // ç›®æ ‡è¿ç»­ç¼“å†²åŒº
    CustomUnpackMeta* meta_list;   // å…ƒæ•°æ®åˆ—è¡¨
    int meta_count;                // å…ƒæ•°æ®æ•°é‡
    cudaStream_t stream;           // CUDAæµ
};
```

#### 2.2 CUDAå†…æ ¸è®¾è®¡
```cuda
__global__ void customUnpackKernel(
    void* dst_buffer,
    CustomUnpackMeta* meta_list,
    int meta_count
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= meta_count) return;
    
    CustomUnpackMeta meta = meta_list[idx];
    
    // æ‰§è¡Œå†…å­˜å¤åˆ¶ï¼šä»åˆ†æ•£é¡µé¢åˆ°è¿ç»­ç¼“å†²åŒº
    memcpy_async(
        (char*)dst_buffer + meta.dst_offset,
        (char*)meta.src_page_ptr + meta.src_offset,
        meta.length
    );
}
```

### **é˜¶æ®µ3: é›†æˆåˆ°TCPXæµ‹è¯• (2-3å¤©)**

#### 3.1 ä¿®æ”¹æµ‹è¯•ä»£ç 
- [ ] åœ¨ `tcpxItest` å®Œæˆåè·å–unpackå…ƒæ•°æ®
- [ ] è°ƒç”¨è‡ªå®šä¹‰unpackå†…æ ¸
- [ ] éªŒè¯æ•°æ®å®Œæ•´æ€§

#### 3.2 å…ƒæ•°æ®æå–æ¥å£
```c
// ä»TCPXè¯·æ±‚ä¸­æå–unpackå…ƒæ•°æ®
int extractUnpackMetadata(
    struct tcpxRequest* request,
    CustomUnpackMeta** meta_list,
    int* meta_count
);

// æ‰§è¡Œè‡ªå®šä¹‰unpackæ“ä½œ
int executeCustomUnpack(
    void* dst_buffer,
    CustomUnpackMeta* meta_list,
    int meta_count,
    cudaStream_t stream
);
```

### **é˜¶æ®µ4: æ€§èƒ½ä¼˜åŒ–ä¸æµ‹è¯• (1-2å¤©)**

#### 4.1 æ€§èƒ½ä¼˜åŒ–
- [ ] ä¼˜åŒ–CUDAå†…æ ¸å‚æ•°
- [ ] å®ç°å¼‚æ­¥æ‰§è¡Œ
- [ ] æ·»åŠ é”™è¯¯å¤„ç†

#### 4.2 å…¨é¢æµ‹è¯•
- [ ] å•å…ƒæµ‹è¯•ï¼šéªŒè¯unpackå†…æ ¸æ­£ç¡®æ€§
- [ ] é›†æˆæµ‹è¯•ï¼šç«¯åˆ°ç«¯TCPXä¼ è¾“
- [ ] æ€§èƒ½æµ‹è¯•ï¼šä¸æ ‡å‡†NCCLå¯¹æ¯”

## ğŸ”§ **éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶**

### 1. **æµ‹è¯•ä»£ç ä¿®æ”¹**
- `p2p/tcpx/tests/test_tcpx_transfer.cc`
  - æ·»åŠ è‡ªå®šä¹‰unpackå†…æ ¸è°ƒç”¨
  - ä¿®æ”¹éªŒè¯é€»è¾‘

### 2. **æ–°å¢æ–‡ä»¶**
- `p2p/tcpx/src/custom_unpack.cu` - è‡ªå®šä¹‰unpack CUDAå†…æ ¸
- `p2p/tcpx/src/custom_unpack.h` - æ¥å£å®šä¹‰
- `p2p/tcpx/src/metadata_extractor.cc` - å…ƒæ•°æ®æå–å·¥å…·

### 3. **æ„å»ºç³»ç»Ÿä¿®æ”¹**
- `p2p/tcpx/CMakeLists.txt` - æ·»åŠ CUDAç¼–è¯‘æ”¯æŒ

## ğŸ¯ **æˆåŠŸæ ‡å‡†**

1. **åŠŸèƒ½éªŒè¯**: è‡ªå®šä¹‰unpackå†…æ ¸èƒ½æ­£ç¡®å°†åˆ†æ•£æ•°æ®å¤åˆ¶åˆ°è¿ç»­ç¼“å†²åŒº
2. **æ•°æ®å®Œæ•´æ€§**: æ¥æ”¶åˆ°çš„æ•°æ®ä¸å‘é€çš„æ•°æ®å®Œå…¨ä¸€è‡´
3. **æ€§èƒ½åŸºå‡†**: ä¼ è¾“æ€§èƒ½æ¥è¿‘æˆ–è¶…è¿‡æ ‡å‡†NCCLå®ç°
4. **ç¨³å®šæ€§**: åœ¨å„ç§æ•°æ®å¤§å°å’Œæ¨¡å¼ä¸‹ç¨³å®šå·¥ä½œ

## ğŸš€ **ä¸‹ä¸€æ­¥è¡ŒåŠ¨**

1. **ç«‹å³å¼€å§‹**: æ·±å…¥ç ”ç©¶NCCL unpackå†…æ ¸æºç 
2. **å¹¶è¡Œè¿›è¡Œ**: åˆ†æTCPXæ’ä»¶çš„unpacké˜Ÿåˆ—æœºåˆ¶
3. **å¿«é€ŸåŸå‹**: å®ç°åŸºç¡€çš„è‡ªå®šä¹‰unpackå†…æ ¸
4. **è¿­ä»£ä¼˜åŒ–**: æ ¹æ®æµ‹è¯•ç»“æœæŒç»­æ”¹è¿›

è¿™ä¸ªè®¡åˆ’å°†ä½¿æˆ‘ä»¬èƒ½å¤ŸçœŸæ­£ç†è§£å’Œè§£å†³TCPX GPUæ¥æ”¶è·¯å¾„çš„é—®é¢˜ï¼Œå®ç°é«˜æ€§èƒ½çš„GPU Directç½‘ç»œä¼ è¾“ã€‚

## ğŸ“š **æŠ€æœ¯å‚è€ƒèµ„æ–™**

### **NCCLç›¸å…³**
- NCCL unpackå†…æ ¸: `https://github.com/NVIDIA/nccl/tree/master/src/device/network/unpack`
- NCCL primitives: `https://github.com/NVIDIA/nccl/blob/master/src/device/prims_simple.h#L242`
- NCCLè®¾å¤‡ä»£ç : `https://github.com/NVIDIA/nccl/tree/master/src/device`

### **Linux Kernel DevMem-TCP**
- å®˜æ–¹æ–‡æ¡£: `https://docs.kernel.org/networking/devmem.html`
- å†…æ ¸å®ç°: `https://www.kernel.org/doc/Documentation/networking/`
- æµ‹è¯•ä»£ç : `tools/testing/selftests/drivers/net/hw/ncdevmem.c`

### **TCPXæ’ä»¶**
- Googleå®ç°: `https://github.com/google/nccl-plugin-gpudirecttcpx`
- æ¥æ”¶è·¯å¾„: `src/sock/tcpx.h:230` (gpudirectTCPXRecv)
- æ§åˆ¶æ¶ˆæ¯å¤„ç†: `src/sock/tcpx.h:136` (process_recv_cmsg)

## ğŸ” **å…³é”®æ•°æ®ç»“æ„åˆ†æ**

### **1. TCPX LoadMetaç»“æ„**
```c
// åŸºäºTCPXæ’ä»¶æºç åˆ†æ
union loadMeta {
    struct {
        uint32_t src_off;    // æºåç§»é‡
        uint32_t len;        // æ•°æ®é•¿åº¦
        uint64_t dst_off;    // ç›®æ ‡åç§»é‡
    };
    // å¯èƒ½è¿˜æœ‰å…¶ä»–å­—æ®µ...
};
```

### **2. Linux DevMemç»“æ„**
```c
// åŸºäºLinuxå†…æ ¸æ–‡æ¡£
struct dmabuf_cmsg {
    __u32 frag_offset;   // ç‰‡æ®µåœ¨dmabufä¸­çš„åç§»
    __u32 frag_size;     // ç‰‡æ®µå¤§å°
    __u32 frag_token;    // ç”¨äºé‡Šæ”¾çš„ä»¤ç‰Œ
    __u32 dmabuf_id;     // dmabufæ ‡è¯†ç¬¦
};
```

### **3. TCPX Unpack Slot**
```c
// åŸºäºTCPXæ’ä»¶æºç 
struct unpackSlot {
    void* mem;              // unpackå…ƒæ•°æ®å†…å­˜
    uint64_t idx;           // é˜Ÿåˆ—ç´¢å¼•
    size_t cnt_cache;       // ç¼“å­˜è®¡æ•°
    bool active;            // æ˜¯å¦æ´»è·ƒ
    // å…¶ä»–å­—æ®µ...
};
```

# TCPX Device-Unpack Architecture (Plan A)

Goal
- Reuse NCCLâ€™s network unpack kernel logic to gather a scatter-list of received TCPX packet buffers directly into a user-provided contiguous GPU buffer.
- Bypass NCCLâ€™s higher-level device kernels (do NOT use `prims_simple.h` or NCCL scheduling). We will adapt the unpack kernel interface and provide our own descriptor structures.
- Keep a host-recv fallback for bring-up and debug.

Context
- In `nccl-plugin-gpudirecttcpX`, `tcpxIrecv_v5` does not write directly into the userâ€™s `void* data` when using CUDA buffers. NCCL typically runs a device-side â€œunpackâ€ kernel that copies from a scatter-list (delivered via devmem-tcp ancillary data) into user memory.
- Upstream references shared by GCP authors:
  - NCCL unpack kernels: `src/device/network/unpack/` (NCCL repo)
  - NCCL kernels call sites: `src/device/prims_simple.h` (we will not use this; only the unpack logic is needed)
  - devmem-tcp ancillary (cmsg) carries scatter segments to userspace; Googleâ€™s plugin references: `src/sock/tcpx.h`
  - Upstream kernel doc: `Documentation/networking/devmem.rst`

What We Will Build
1) A device-side unpack kernel (ported/adapted from NCCLâ€™s network/unpack) that reads a GPU-visible descriptor array describing the packet fragments, and writes into the contiguous destination buffer.
2) A host-side pipeline that, upon `tcpxTest` completion, retrieves or derives the scatter-list for a completed receive and builds GPU descriptors, then launches the unpack kernel, and finally calls `tcpxIrecvConsumed`.
3) A host-recv fallback (NCCL_PTR_HOST) for environments where devmem-tcp/GPU path is not ready.

Key Design Choices
- No NCCL dependency: we will copy/adapt the minimal code we need into `p2p/tcpx/device/` and define our own descriptor types in `p2p/tcpx/rx/`.
- Two sources for scatter information (implementation-time decision):
  - Preferred: consume the devmem-tcp scatter from ancillary data (cmsg) associated with the recvâ€™d request (see `nccl-plugin-gpudirecttcpx/src/sock/tcpx.h` for how the plugin interprets cmsg). This requires a hook/adapter in our userspace to access what the plugin already parsed or to replicate that parsing.
  - Alternative (depending on plugin internals): use `tcpxGetDeviceHandle` (queue exported to GPU) and derive fragment positions from the device queue metadata if exposed. This option depends on the pluginâ€™s device-queue API stability.

File/Module Layout (to be added)
- `p2p/tcpx/rx/`
  - `rx_cmsg_parser.h/.cc`: Parse devmem-tcp cmsg (ancillary data) into a normalized host scatter-list.
  - `rx_descriptor.h`: GPU-visible descriptor PODs (aligned) and conversion helpers.
  - `rx_staging.h/.cc`: Manage pinned host staging for descriptors or device copies of descriptor blocks.
- `p2p/tcpx/device/`
  - `unpack_kernels.cu`: Adapted NCCL unpack logic (vectorized copies from fragments â†’ dst buffer).
  - `unpack_launch.h/.cc`: Kernel launch, stream selection, basic error prop.
- `p2p/tcpx/pipeline/`
  - `rx_pipeline.h/.cc`: Orchestrates irecv â†’ test(done) â†’ parse/derive scatter â†’ build descriptors â†’ launch unpack â†’ record CUDA event â†’ irecvConsumed.
  - `opts.h`: Env flags and tunables.
- `p2p/tcpx/reference/unpack/` (already present)
  - `unpack.h`, `unpack_defs.h`: Source references for porting (read-only references; do not depend on NCCL headers at compile time).

Existing Files We Will Touch Integrate With (no code change in this step)
- `nccl-plugin-gpudirecttcpx/src/net_tcpx.cc` and `.../src/sock/tcpx.h`: Guidance for how cmsg/devmem-tcp scatter is represented.
- `nccl-plugin-gpudirecttcpx/src/net_tcpx.h`: API surface. We may add wrappers in our interface for `getDeviceHandle`/`getDeviceMr` if needed.
- `p2p/tcpx/tcpx_impl.cc` and `p2p/tcpx/tcpx_interface.h`: Will eventually expose a high-level â€œdevice-unpack receiveâ€ operation (or a flag to turn it on) without changing public test APIs yet.
- `p2p/tcpx/tests/test_tcpx_transfer.cc`: Once the pipeline exists, switch the CUDA receive path to â€œpost â†’ unpack â†’ validateâ€ using our pipeline (host fallback remains for debug).

Descriptor Specifications
Host-normalized scatter list (built from cmsg):
```c
// Host side (neutral format)
typedef struct {
  uint64_t dev_addr;  // device-visible source pointer from devmem-tcp
  uint32_t len;
  uint32_t _pad;
} RxFrag;

typedef struct {
  uint32_t nfrags;
  uint32_t total_len;
  RxFrag   frags[MAX_FRAGS];
} RxList;
```

GPU-visible descriptors (what the kernel consumes):
```c
// Device side (contiguous descriptors copied/mapped to GPU)
typedef struct {
  const uint8_t* src; // device-visible src pointer (from dev_addr)
  uint32_t       len;
  uint32_t       dst_off; // offset within dst buffer
} GpuFrag;

typedef struct {
  uint32_t nfrags;
  uint32_t total_len;
  // Followed by nfrags entries
  GpuFrag  frags[];
} GpuList;
```

Kernel Interface (adapted)
```c++
// device/unpack_kernels.cu
__global__ void tcpx_unpack_kernel(const GpuFrag* __restrict__ list,
                                   uint8_t* __restrict__ dst,
                                   int nfrags) {
  // Baseline: per-fragment CTAs or per-chunk grid; vectorized loads/stores.
  // Copy list[fi].src[0..len) â†’ dst + list[fi].dst_off
}
```

Receive Pipeline (Server)
1) `tcpxIrecv_v5(recv_comm, ..., size=payload, tag)` â€” as today.
2) Poll `tcpxTest` until `done`.
3) Obtain scatter list for this request:
   - Preferred: parse devmem-tcp cmsg associated with the receive (see `sock/tcpx.h` patterns). If the plugin hides the cmsg fully, add a thin adapter to expose the parsed result.
   - Alternative: derive from device queue handle (`tcpxGetDeviceHandle`) if the queue exposes fragment metadata in device memory accessible to our kernel.
4) Build `GpuList` with dst offsets; copy/make visible to GPU.
5) Launch `tcpx_unpack_kernel` on a CUDA stream; record event.
6) On event completion: call `tcpxIrecvConsumed(recv_comm, 1, request)`; signal upper layer (and send app-level ACK if used).

Send Pipeline (Client)
- Unchanged: `tcpxRegMr(CUDA)` â†’ payload upload to device â†’ `tcpxIsend` â†’ `tcpxTest`.
- Ensure a fence after the payload upload (as we already do) before starting zero-copy.

Fallback (Host-Staged)
- If `NIXL_TCPX_USE_DEVUNPACK=0` or no devmem support:
  - Receive into page-aligned host memory (NCCL_PTR_HOST) and memcpy/cudaMemcpyHtoD to the user dst buffer.

Env/Tunables
- `NIXL_TCPX_USE_DEVUNPACK=1` (default in production)
- `NIXL_TCPX_MAX_FRAGS` (bounds on descriptors; coalesce if exceeded)
- `NIXL_TCPX_COALESCE_THRESHOLD` (bytes; small fragments can be merged by staging)
- `NCCL_MIN_ZCOPY_SIZE`, `NCCL_GPUDIRECTTCPX_MIN_ZCOPY_SIZE` (transport behavior)
- `NCCL_GPUDIRECTTCPX_RECV_SYNC` (useful during debug)
- `NIXL_TCPX_DEBUG=1` (pipeline logs)

Implementation Steps (Milestones)
1) Kernel Port (no NCCL includes):
   - Copy minimal logic from `p2p/tcpx/reference/unpack/` into `device/unpack_kernels.cu`.
   - Replace NCCL types/macros with our own in `rx_descriptor.h`.
   - Provide a simple kernel that iterates `GpuFrag` entries and performs vectorized copies.
2) Descriptor Path:
   - Implement `rx_cmsg_parser` to normalize devmem-tcp scatter into `RxList`.
   - Implement conversion to `GpuList` with computed `dst_off`.
   - Implement `rx_staging` to place descriptors in pinned memory (or cudaMemcpy to device).
3) Launch + Orchestration:
   - Implement `unpack_launch` to launch kernels and manage streams/events.
   - Implement `rx_pipeline` to glue: irecv â†’ test â†’ parse â†’ build â†’ launch â†’ event â†’ irecvConsumed.
4) Integrate With Tests:
   - In `tests/test_tcpx_transfer.cc`, for CUDA receive path, replace direct `cuMemcpyDtoH` verification with: wait â†’ run `rx_pipeline` â†’ verify dst buffer â†’ ACK.
   - Keep host-recv toggle for A/B.
5) Integrate With UCCL/NIXL:
   - Add a thin adapter (e.g., `nixl_tcpx_engine`) that uses `rx_pipeline` for receives under the NIXL API exposed by `uccl_engine.h`.
6) Validation + Perf:
   - Start with large payloads, verify correctness, add metrics, then tune kernel.

Notes on Accessing Scatter Data
- The devmem-tcp cmsg is the authoritative source for packet fragment locations (as pointed by GCP authors). If the current plugin encapsulates cmsg parsing, we will add a small adapter at the plugin/user boundary to surface a normalized scatter list. As an alternative, if the pluginâ€™s device queue already holds the scatter metadata in GPU-visible memory, we can read from that in our kernel directly (requires confirming `tcpxGetDeviceHandle` semantics).

Risks & Mitigations
- Kernel ABI drift (devmem-tcp): abstract via `rx_cmsg_parser` with compile-time guards.
- Many tiny fragments: coalesce on host or stage into a temp device buffer to reduce descriptor count.
- Synchronization: maintain fences before send and after receive completion; verify visibility with tests.

Deliverables (This Phase)
- This document, adapted kernel interfaces, descriptor specs, and a clear file map.
- No code changes yet; next task is to add the skeleton files above and wire the pipeline under a feature flag (æ–¹æ¡ˆ A).
