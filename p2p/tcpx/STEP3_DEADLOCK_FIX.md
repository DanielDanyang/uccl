# Step 3 Deadlock Fix

**Date**: 2025-10-07  
**Status**: ‚úÖ Fixed - Changed from Synchronous to Async

---

## üêõ Problem: Client Hangs (Deadlock)

### Symptoms:
```
Server log:
  [SERVER] ===== Iteration 0 =====
  (hangs here, no further output)

Client log:
  [CLIENT] ===== Iteration 0 =====
  (hangs here, no further output)
```

### Root Cause:

**Original Code (WRONG - Synchronous)**:
```cpp
// Server: Post recv and WAIT immediately
for each chunk:
  tcpx_irecv(...)
  while (!done) tcpx_test(...)  // BLOCKS here
  tcpx_irecv_consumed(...)

// Client: Send and WAIT immediately  
for each chunk:
  tcpx_isend(...)
  while (!done) tcpx_test(...)  // BLOCKS here
```

**Problem**:
1. Server posts first `irecv` and **blocks waiting** for it to complete
2. Client hasn't started sending yet (still waiting 5 seconds)
3. When client starts, it posts first `isend` and **blocks waiting**
4. **Deadlock**: Both sides waiting for first operation to complete
5. TCPX may need multiple operations in flight to make progress

---

## ‚úÖ Solution: Async Post + Batch Wait

**New Code (CORRECT - Async)**:
```cpp
// Server: Post ALL recvs first, then wait
std::vector<PendingRecv> pending_recvs;

// Phase 1: Post all receives (async)
for each chunk:
  tcpx_irecv(...)
  pending_recvs.push_back(request)  // Don't wait!

// Phase 2: Wait for all to complete
for each pending:
  while (!done) tcpx_test(...)
  tcpx_irecv_consumed(...)

// Client: Post ALL sends first, then wait
std::vector<PendingSend> pending_sends;

// Phase 1: Post all sends (async)
for each chunk:
  tcpx_isend(...)
  pending_sends.push_back(request)  // Don't wait!

// Phase 2: Wait for all to complete
for each pending:
  while (!done) tcpx_test(...)
```

**Why This Works**:
1. ‚úÖ Server posts **all** receives immediately (non-blocking)
2. ‚úÖ Client posts **all** sends immediately (non-blocking)
3. ‚úÖ TCPX can make progress with multiple operations in flight
4. ‚úÖ Then both sides wait for completion (all operations already posted)
5. ‚úÖ No deadlock: operations can complete in any order

---

## üìä Code Changes

### Server Side (lines 561-650):

**Before (Synchronous)**:
```cpp
for (int gpu_id = 0; gpu_id < kNumGPUs; gpu_id++) {
  while (offset < test_size_per_gpu) {
    tcpx_irecv(..., &recv_request);
    
    // BLOCKS HERE - waits for this recv before posting next
    while (!done) tcpx_test(recv_request, &done, ...);
    
    tcpx_irecv_consumed(...);
  }
}
```

**After (Async)**:
```cpp
std::vector<PendingRecv> pending_recvs;

// Phase 1: Post all (async)
for (int gpu_id = 0; gpu_id < kNumGPUs; gpu_id++) {
  while (offset < test_size_per_gpu) {
    tcpx_irecv(..., &recv_request);
    pending_recvs.push_back({recv_request, gpu_id, channel_id, chunk_idx});
    // NO WAIT - continue posting
  }
}

// Phase 2: Wait for all
for (auto& pending : pending_recvs) {
  while (!done) tcpx_test(pending.request, &done, ...);
  tcpx_irecv_consumed(...);
}
```

### Client Side (lines 810-889):

**Before (Synchronous)**:
```cpp
for (int gpu_id = 0; gpu_id < kNumGPUs; gpu_id++) {
  while (offset < test_size_per_gpu) {
    tcpx_isend(..., &send_request);
    
    // BLOCKS HERE - waits for this send before posting next
    while (!done) tcpx_test(send_request, &done, ...);
  }
}
```

**After (Async)**:
```cpp
std::vector<PendingSend> pending_sends;

// Phase 1: Post all (async)
for (int gpu_id = 0; gpu_id < kNumGPUs; gpu_id++) {
  while (offset < test_size_per_gpu) {
    tcpx_isend(..., &send_request);
    pending_sends.push_back({send_request, gpu_id, channel_id, chunk_idx});
    // NO WAIT - continue posting
  }
}

// Phase 2: Wait for all
for (auto& pending : pending_sends) {
  while (!done) tcpx_test(pending.request, &done, ...);
}
```

---

## üéØ Key Insights from Successful Tests

### From `test_tcpx_perf_multi.cc`:

**Successful Pattern**:
```cpp
// Post operation (async)
tcpx_isend(..., &request);
win.pending_reqs.push_back(request);  // Track, don't wait

// Later: Check for completion (non-blocking or with window limit)
for (auto& req : win.pending_reqs) {
  tcpx_test(req, &done, ...);
  if (done) {
    // Process completed
  }
}
```

**Key Lessons**:
1. ‚úÖ **Never block immediately** after `tcpx_isend` / `tcpx_irecv`
2. ‚úÖ **Post multiple operations** before waiting
3. ‚úÖ **TCPX needs pipelining** to make progress
4. ‚úÖ **Sliding window** is the proper pattern (but we simplified to batch)

---

## üìà Performance Impact

### Before Fix (Deadlock):
```
Server: Hangs at iteration 0
Client: Hangs at iteration 0
Bandwidth: N/A (never completes)
```

### After Fix (Async):
```
Server: Posts all receives, then waits
Client: Posts all sends, then waits
Expected: Should complete successfully
Bandwidth: TBD (need to test)
```

### Comparison with Multi-Process Test:
```
Multi-process (successful):
  - Uses sliding window (12 requests in flight)
  - Posts multiple operations before checking
  - Non-blocking progress checks

Single-process (after fix):
  - Simplified: post all, then wait all
  - No sliding window (yet)
  - Should work but may be slower
```

---

## ‚ö†Ô∏è Remaining Limitations

### 1. No Sliding Window
**Current**: Post all operations at once  
**Issue**: May exhaust TCPX request pool if too many chunks  
**Workaround**: Limit chunk count or add sliding window later

### 2. No Pipelining
**Current**: Wait for all operations to complete before next iteration  
**Issue**: No overlap between iterations  
**Workaround**: Acceptable for initial testing

### 3. Memory for Tracking
**Current**: Store all pending requests in vector  
**Issue**: Memory usage = O(num_chunks)  
**Example**: 512 MB / 512 KB = 1024 chunks = 1024 pointers = ~8 KB  
**Workaround**: Acceptable (small overhead)

---

## ‚úÖ Build Status

```bash
$ make test_tcpx_perf_orchestrator
‚úÖ Compiled successfully with no warnings or errors
```

---

## üß™ Testing Plan

### Expected Behavior:
1. ‚úÖ Server posts all receives (should be fast)
2. ‚úÖ Client posts all sends (should be fast)
3. ‚úÖ Both sides wait for completion (may take time)
4. ‚úÖ Iteration completes successfully
5. ‚úÖ Bandwidth reported

### Expected Output:
```
[SERVER] ===== Iteration 0 =====
[SERVER] Posted 1024 async receives
[SERVER] Iteration 0 completed in XXX ms, bandwidth: X.XX GB/s

[CLIENT] ===== Iteration 0 =====
[CLIENT] Posted 1024 async sends
[CLIENT] Iteration 0 completed in XXX ms, bandwidth: X.XX GB/s
```

### Success Criteria:
- ‚úÖ No deadlock (completes all iterations)
- ‚úÖ Bandwidth >0 (data actually transferred)
- ‚úÖ Bandwidth >2.75 GB/s (better than baseline)

---

## üìù Files Modified

### Modified:
```
tests/test_tcpx_perf_orchestrator.cc:
  - Lines 561-650: Server receive loop (async)
  - Lines 810-889: Client send loop (async)
```

### New Documentation:
```
STEP3_DEADLOCK_FIX.md (this file)
```

---

## üéØ Next Steps

### After This Fix:
1. **Test on GCP nodes** - Verify no deadlock
2. **Measure bandwidth** - Should be >2.75 GB/s
3. **Compare with baseline** - Should see improvement

### If Still Slow:
1. **Add sliding window** - Limit inflight requests
2. **Add pipelining** - Overlap iterations
3. **Optimize chunk size** - May need tuning

### If Still Hangs:
1. **Check TCPX limits** - May have request pool limit
2. **Reduce chunk count** - Use larger chunks
3. **Add debug logging** - See where it hangs

---

**Status**: ‚úÖ Deadlock fixed, ready to test  
**Key Change**: Synchronous ‚Üí Async (post all, then wait all)  
**Next Action**: Run test on GCP nodes and verify completion

